{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2025/01/02 讀書會\n",
    "### introduction of pipline +  decoding strategy ( ex: do_sample, max_new_token, … )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # 如果是 True，表示支持 GPU\n",
    "print(torch.cuda.device_count())  # 顯示可用的 GPU 數量\n",
    "print(torch.cuda.get_device_name(0))  # 顯示 GPU 的名稱\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user_data/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/user_data/.local/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/usr/local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='315' max='315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [315/315 03:19, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.359000</td>\n",
       "      <td>0.292282</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>0.891991</td>\n",
       "      <td>0.892697</td>\n",
       "      <td>0.892000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.199500</td>\n",
       "      <td>0.298173</td>\n",
       "      <td>0.886000</td>\n",
       "      <td>0.885806</td>\n",
       "      <td>0.890071</td>\n",
       "      <td>0.886000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.187300</td>\n",
       "      <td>0.375941</td>\n",
       "      <td>0.876000</td>\n",
       "      <td>0.875128</td>\n",
       "      <td>0.889475</td>\n",
       "      <td>0.876000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.058600</td>\n",
       "      <td>0.408484</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>0.904006</td>\n",
       "      <td>0.904128</td>\n",
       "      <td>0.904000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.045600</td>\n",
       "      <td>0.445646</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.900005</td>\n",
       "      <td>0.900038</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "# 1. 載入數據集\n",
    "dataset = load_dataset(\"imdb\")  # IMDB情感分析數據集\n",
    "\n",
    "# 2. 加載預訓練標記器\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 3. 數據預處理\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# 4. 加載預訓練模型\n",
    "num_labels = 2  # 兩個分類（正向、負向）\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},  # 自定義標籤名稱\n",
    "    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1},\n",
    ")\n",
    "\n",
    "model.config.hidden_dropout_prob = 0.3\n",
    "\n",
    "\n",
    "# 5. 訓練參數設置\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",           # 模型保存路徑\n",
    "    evaluation_strategy=\"epoch\",      # 每個 epoch 評估\n",
    "    save_strategy=\"epoch\",            # 每個 epoch 保存\n",
    "    learning_rate=3e-5,               # 學習率\n",
    "    lr_scheduler_type=\"linear\",       # 線性衰減\n",
    "    per_device_train_batch_size=16,   # 訓練批次大小\n",
    "    per_device_eval_batch_size=16,    # 驗證批次大小\n",
    "    num_train_epochs=5,               # 訓練輪數\n",
    "    weight_decay=0.01,                # 權重衰減\n",
    "    logging_dir='./logs',             # 日誌保存路徑\n",
    "    logging_steps=10,                 # 每 10 步記錄一次\n",
    "    load_best_model_at_end=True,      # 儲存最佳模型\n",
    "    metric_for_best_model=\"accuracy\", # 以準確率為判斷基準\n",
    "    fp16=True                         # 啟用混合精度\n",
    ")\n",
    "\n",
    "# 6. 評估指標計算函數\n",
    "def compute_metrics(eval_pred):\n",
    "    from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    precision = precision_score(labels, predictions, average='weighted')\n",
    "    recall = recall_score(labels, predictions, average='weighted')\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "\n",
    "# 7. 初始化 Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"].shuffle(seed=42).select(range(2000)),  # 使用部分數據加速訓練\n",
    "    eval_dataset=encoded_dataset[\"test\"].shuffle(seed=42).select(range(500)),     # 使用部分數據加速驗證\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"./saved_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9969589710235596}, {'label': 'NEGATIVE', 'score': 0.9957774877548218}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-classification\", model=\"./saved_model\", tokenizer=tokenizer, device=0)\n",
    "print(pipe([\"I love this movie, it was fantastic!\", \"I hate this movie, it was bad!\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9996050000190735}]\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\", device=0)\n",
    "result = pipe(\"This is a terrible product.\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/user_data/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'This is a terrible product. It\\'s just stupid. The packaging design is horrible. It\\'s not a good product. And I really wish they would make sure they made sure this would happen.\"\\n\\nShe said they had no reason to stop with'}]\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", device=0)\n",
    "result = pipe(\"This is a terrible product.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"eos_token_id\": 50256\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
    "model.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the world was a place\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 加載 GPT-2 模型作為生成器cx\n",
    "pipe = pipeline(\"text-generation\", model=\"gpt2\", device=0)  # 使用 GPU（若可用）\n",
    "\n",
    "result = pipe(\n",
    "    \"Once upon a time,\", \n",
    "    truncation=True,  # 自動截斷\n",
    "    max_length=10,    # 最大長度\n",
    "    do_sample=False,  # 是否隨機取樣\n",
    "    num_beams=1       # beam分支數量\n",
    ")\n",
    "print(result[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, I was given the opportunity to speak with a number of experts on the subject. One of them was Professor of Psychology at the University of California, San Diego, and the other was Professor of Psychology at the University of California,\n"
     ]
    }
   ],
   "source": [
    "result = generator(\n",
    "    \"Once upon a time,\", \n",
    "    max_length=50, \n",
    "    num_beams=5,          \n",
    "    early_stopping=True,  # 提前停止\n",
    "    length_penalty=1.2    # 長度偏好\n",
    ")\n",
    "print(result[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, I have always wanted to write about what makes the world tick when we see it.\n",
      "I've been working in an art studio for many years now and my wife recently moved into our new home on Lake Michigan from another city\n"
     ]
    }
   ],
   "source": [
    "result = generator(\n",
    "    \"Once upon a time,\", \n",
    "    min_length=10, \n",
    "    do_sample=True,   \n",
    "    temperature=0.8, \n",
    "    top_k=10,\n",
    "    repetition_penalty=1.2,  # 懲罰重複生成的詞\n",
    "    no_repeat_ngram_size=2   # 防止重複的 bi-gram\n",
    ")\n",
    "print(result[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
