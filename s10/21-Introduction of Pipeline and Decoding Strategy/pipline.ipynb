{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2025/01/02 è®€æ›¸æœƒ\n",
    "### introduction of pipline +  decoding strategy ( ex: do_sample, max_new_token, â€¦ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # å¦‚æœæ˜¯ Trueï¼Œè¡¨ç¤ºæ”¯æŒ GPU\n",
    "print(torch.cuda.device_count())  # é¡¯ç¤ºå¯ç”¨çš„ GPU æ•¸é‡\n",
    "print(torch.cuda.get_device_name(0))  # é¡¯ç¤º GPU çš„åç¨±\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user_data/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/user_data/.local/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/usr/local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='315' max='315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [315/315 03:19, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.359000</td>\n",
       "      <td>0.292282</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>0.891991</td>\n",
       "      <td>0.892697</td>\n",
       "      <td>0.892000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.199500</td>\n",
       "      <td>0.298173</td>\n",
       "      <td>0.886000</td>\n",
       "      <td>0.885806</td>\n",
       "      <td>0.890071</td>\n",
       "      <td>0.886000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.187300</td>\n",
       "      <td>0.375941</td>\n",
       "      <td>0.876000</td>\n",
       "      <td>0.875128</td>\n",
       "      <td>0.889475</td>\n",
       "      <td>0.876000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.058600</td>\n",
       "      <td>0.408484</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>0.904006</td>\n",
       "      <td>0.904128</td>\n",
       "      <td>0.904000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.045600</td>\n",
       "      <td>0.445646</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.900005</td>\n",
       "      <td>0.900038</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "# 1. è¼‰å…¥æ•¸æ“šé›†\n",
    "dataset = load_dataset(\"imdb\")  # IMDBæƒ…æ„Ÿåˆ†ææ•¸æ“šé›†\n",
    "\n",
    "# 2. åŠ è¼‰é è¨“ç·´æ¨™è¨˜å™¨\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 3. æ•¸æ“šé è™•ç†\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# 4. åŠ è¼‰é è¨“ç·´æ¨¡å‹\n",
    "num_labels = 2  # å…©å€‹åˆ†é¡ï¼ˆæ­£å‘ã€è² å‘ï¼‰\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},  # è‡ªå®šç¾©æ¨™ç±¤åç¨±\n",
    "    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1},\n",
    ")\n",
    "\n",
    "model.config.hidden_dropout_prob = 0.3\n",
    "\n",
    "\n",
    "# 5. è¨“ç·´åƒæ•¸è¨­ç½®\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",           # æ¨¡å‹ä¿å­˜è·¯å¾‘\n",
    "    evaluation_strategy=\"epoch\",      # æ¯å€‹ epoch è©•ä¼°\n",
    "    save_strategy=\"epoch\",            # æ¯å€‹ epoch ä¿å­˜\n",
    "    learning_rate=3e-5,               # å­¸ç¿’ç‡\n",
    "    lr_scheduler_type=\"linear\",       # ç·šæ€§è¡°æ¸›\n",
    "    per_device_train_batch_size=16,   # è¨“ç·´æ‰¹æ¬¡å¤§å°\n",
    "    per_device_eval_batch_size=16,    # é©—è­‰æ‰¹æ¬¡å¤§å°\n",
    "    num_train_epochs=5,               # è¨“ç·´è¼ªæ•¸\n",
    "    weight_decay=0.01,                # æ¬Šé‡è¡°æ¸›\n",
    "    logging_dir='./logs',             # æ—¥èªŒä¿å­˜è·¯å¾‘\n",
    "    logging_steps=10,                 # æ¯ 10 æ­¥è¨˜éŒ„ä¸€æ¬¡\n",
    "    load_best_model_at_end=True,      # å„²å­˜æœ€ä½³æ¨¡å‹\n",
    "    metric_for_best_model=\"accuracy\", # ä»¥æº–ç¢ºç‡ç‚ºåˆ¤æ–·åŸºæº–\n",
    "    fp16=True                         # å•Ÿç”¨æ··åˆç²¾åº¦\n",
    ")\n",
    "\n",
    "# 6. è©•ä¼°æŒ‡æ¨™è¨ˆç®—å‡½æ•¸\n",
    "def compute_metrics(eval_pred):\n",
    "    from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    precision = precision_score(labels, predictions, average='weighted')\n",
    "    recall = recall_score(labels, predictions, average='weighted')\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "\n",
    "# 7. åˆå§‹åŒ– Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"].shuffle(seed=42).select(range(2000)),  # ä½¿ç”¨éƒ¨åˆ†æ•¸æ“šåŠ é€Ÿè¨“ç·´\n",
    "    eval_dataset=encoded_dataset[\"test\"].shuffle(seed=42).select(range(500)),     # ä½¿ç”¨éƒ¨åˆ†æ•¸æ“šåŠ é€Ÿé©—è­‰\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"./saved_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9969589710235596}, {'label': 'NEGATIVE', 'score': 0.9957774877548218}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-classification\", model=\"./saved_model\", tokenizer=tokenizer, device=0)\n",
    "print(pipe([\"I love this movie, it was fantastic!\", \"I hate this movie, it was bad!\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9996050000190735}]\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\", device=0)\n",
    "result = pipe(\"This is a terrible product.\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/user_data/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'This is a terrible product. It\\'s just stupid. The packaging design is horrible. It\\'s not a good product. And I really wish they would make sure they made sure this would happen.\"\\n\\nShe said they had no reason to stop with'}]\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", device=0)\n",
    "result = pipe(\"This is a terrible product.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"eos_token_id\": 50256\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
    "model.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the world was a place\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# åŠ è¼‰ GPT-2 æ¨¡å‹ä½œç‚ºç”Ÿæˆå™¨cx\n",
    "pipe = pipeline(\"text-generation\", model=\"gpt2\", device=0)  # ä½¿ç”¨ GPUï¼ˆè‹¥å¯ç”¨ï¼‰\n",
    "\n",
    "result = pipe(\n",
    "    \"Once upon a time,\", \n",
    "    truncation=True,  # è‡ªå‹•æˆªæ–·\n",
    "    max_length=10,    # æœ€å¤§é•·åº¦\n",
    "    do_sample=False,  # æ˜¯å¦éš¨æ©Ÿå–æ¨£\n",
    "    num_beams=1       # beamåˆ†æ”¯æ•¸é‡\n",
    ")\n",
    "print(result[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, I was given the opportunity to speak with a number of experts on the subject. One of them was Professor of Psychology at the University of California, San Diego, and the other was Professor of Psychology at the University of California,\n"
     ]
    }
   ],
   "source": [
    "result = generator(\n",
    "    \"Once upon a time,\", \n",
    "    max_length=50, \n",
    "    num_beams=5,          \n",
    "    early_stopping=True,  # æå‰åœæ­¢\n",
    "    length_penalty=1.2    # é•·åº¦åå¥½\n",
    ")\n",
    "print(result[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, I have always wanted to write about what makes the world tick when we see it.\n",
      "I've been working in an art studio for many years now and my wife recently moved into our new home on Lake Michigan from another city\n"
     ]
    }
   ],
   "source": [
    "result = generator(\n",
    "    \"Once upon a time,\", \n",
    "    min_length=10, \n",
    "    do_sample=True,   \n",
    "    temperature=0.8, \n",
    "    top_k=10,\n",
    "    repetition_penalty=1.2,  # æ‡²ç½°é‡è¤‡ç”Ÿæˆçš„è©\n",
    "    no_repeat_ngram_size=2   # é˜²æ­¢é‡è¤‡çš„ bi-gram\n",
    ")\n",
    "print(result[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
