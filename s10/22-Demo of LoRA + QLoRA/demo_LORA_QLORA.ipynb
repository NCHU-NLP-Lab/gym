{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Llama Model on Arxiv Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check torch GPU\n",
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "\n",
    "# # Download latest version\n",
    "# path = kagglehub.dataset_download(\"Cornell-University/arxiv\", path='Cornell-University/arxiv')\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load necessary modules and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, BitsAndBytesConfig, pipeline\n",
    "from trl import DataCollatorForCompletionOnlyLM, SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "BASE_MODEL = \"Llama-3.2-3B-Instruct\"\n",
    "SUBSET_RATIO = 0.01  # 使用 1% 的資料\n",
    "\n",
    "\n",
    "# Paths\n",
    "BASE_MODEL_DIR = os.path.join(\"./models/\", BASE_MODEL)\n",
    "DATASET = \"./data/Cornell-University/arxiv\"\n",
    "DATASET_MAP = \"./data/Cornell-University/arxiv_tokenized_dataset\"\n",
    "OUTPUT_DIR = os.path.join(\"./fine_tuned_models/\", f\"{BASE_MODEL}_{SUBSET_RATIO}\")\n",
    "\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
    "\n",
    "print(tokenizer.pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(DATASET, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_dataset = dataset.shuffle(seed=42).select(range(int(len(dataset) * SUBSET_RATIO)))\n",
    "print(f\"使用的資料集大小: {len(subset_dataset)} 個樣本\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['abstract'])):\n",
    "        text = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are an AI assistant to summarize scientific abstracts into concise and accurate title.\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "{example['abstract'][i]}\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "{example['title'][i]}<|eot_id|>\"\"\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. FineTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "response_template = \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template=response_template, tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload base model for LoRA \n",
    "lora_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_DIR, device_map=\"auto\")\n",
    "lora_model.gradient_checkpointing_enable()\n",
    "lora_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Set LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Apply LoRA to the base model\n",
    "lora_model = get_peft_model(lora_model, lora_config)\n",
    "\n",
    "# Set LoRA training args\n",
    "lora_training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(OUTPUT_DIR, \"lora\"),\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,  # 累積梯度\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "# Initialize LoRA Trainer\n",
    "lora_trainer = SFTTrainer(\n",
    "    model=lora_model,\n",
    "    args=lora_training_args,\n",
    "    train_dataset=subset_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,  # 動態填充\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    optimizers=(bnb.optim.Adam8bit(lora_model.parameters(), lr=1e-4), None)  # 使用 8-bit Adam 優化器，減少記憶體使用 (預設使用 AdamW)\n",
    ")\n",
    "\n",
    "# Train LoRA model\n",
    "lora_trainer.train()\n",
    "lora_trainer.save_model(os.path.join(OUTPUT_DIR,\"lora/final_model\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty VRAM\n",
    "del lora_model\n",
    "# del pipe\n",
    "del lora_trainer\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define quantization config for QLoRA\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load 4-bit QLoRA model\n",
    "qlora_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_DIR,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "qlora_model.gradient_checkpointing_enable()\n",
    "qlora_model.resize_token_embeddings(len(tokenizer))\n",
    "qlora_model = prepare_model_for_kbit_training(qlora_model, use_gradient_checkpointing = False)\n",
    "\n",
    "# Set QLoRA config\n",
    "qlora_config = LoraConfig(\n",
    "    r=64,  # Typically higher for QLoRA\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Apply QLoRA to the base model\n",
    "qlora_model = get_peft_model(qlora_model, qlora_config)\n",
    "\n",
    "# Ensure only floating point parameters require gradients\n",
    "for param in qlora_model.parameters():\n",
    "    if param.dtype in [torch.bfloat16, torch.float32, torch.float64, torch.complex64, torch.complex128]:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Set QLoRA training args\n",
    "qlora_training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(OUTPUT_DIR, \"qlora\"),\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,  # 累積梯度\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "# Initialize QLoRA Trainer\n",
    "qlora_trainer = SFTTrainer(\n",
    "    model=qlora_model,\n",
    "    args=qlora_training_args,\n",
    "    train_dataset=subset_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,  # 動態填充\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    optimizers=(bnb.optim.Adam8bit(qlora_model.parameters(), lr=1e-4), None)  # 使用 8-bit Adam 優化器，減少記憶體使用 (預設使用 AdamW)\n",
    ")\n",
    "\n",
    "# Train QLoRA model\n",
    "qlora_trainer.train()\n",
    "qlora_trainer.save_model(os.path.join(OUTPUT_DIR,\"qlora/final_model\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty VRAM\n",
    "del qlora_model\n",
    "# del pipe\n",
    "del qlora_trainer\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "abstract = \"\"\"Creating high-quality True-False (TF) multiple-choice questions (MCQs), with accurate distractors, is a challenging and time-consuming task in education. This paper introduces True-False Distractor Generation (TFDG), a pipeline that leverages pre-trained language models and sentence retrieval techniques to automate the generation of TF-type MCQ distractors. Furthermore, the evaluation of generated TF questions presents a challenge. Traditional metrics like BLEU and ROUGE are unsuitable for this task. To address this, we propose a new evaluation metric called Retrieval-based Accuracy Differential (RAD). RAD assesses the discriminative power of TF questions by comparing model accuracy with and without access to reference texts. It quantitatively evaluates how well questions differentiate between students with varying knowledge levels. This research benefits educators and assessment developers, facilitating the efficient automatic generation of high-quality TF-type MCQs and their reliable evaluation.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are an AI assistant to summarize scientific abstracts into concise and accurate title.\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "{abstract}\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "\n",
    "print(\"[abstract]:\", abstract)\n",
    "\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"[Base model]\")\n",
    "result = pipeline(task=\"text-generation\", model=BASE_MODEL_DIR, tokenizer=tokenizer, max_length=max_length)(prompt)[0]['generated_text']\n",
    "print(\"[Response]:\")\n",
    "print(result[result.find(response_template) + len(response_template):])\n",
    "\n",
    "print(\"-\"*100)\n",
    "\n",
    "print(\"[LoRA model]\")\n",
    "result = pipeline(task=\"text-generation\", model=os.path.join(OUTPUT_DIR, f\"lora/final_model\"), tokenizer=tokenizer, max_length=max_length)(prompt)[0]['generated_text']\n",
    "print(\"[Response]:\")\n",
    "print(result[result.find(response_template) + len(response_template):])\n",
    "\n",
    "print(\"-\"*100)\n",
    "\n",
    "print(\"[QLoRA model]\")\n",
    "result = pipeline(task=\"text-generation\", model=os.path.join(OUTPUT_DIR, \"qlora/final_model\"), tokenizer=tokenizer, max_length=max_length)(prompt)[0]['generated_text']\n",
    "print(\"[Response]:\")\n",
    "print(result[result.find(response_template) + len(response_template):])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
